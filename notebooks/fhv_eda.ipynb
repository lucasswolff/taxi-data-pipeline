{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get an existing Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EDA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 17630326\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2024-12-01 00:19:00|2024-12-01 00:42:00|        NULL|        NULL|   NULL|                B00009|\n",
      "|              B00009|2024-12-01 00:00:00|2024-12-01 00:18:00|        NULL|        NULL|   NULL|                B00009|\n",
      "|              B00009|2024-12-01 00:00:00|2024-12-01 00:27:00|        NULL|        NULL|   NULL|                B00009|\n",
      "|              B00009|2024-12-01 00:39:00|2024-12-01 01:46:00|        NULL|        NULL|   NULL|                B00009|\n",
      "|              B00111|2024-12-01 00:02:00|2024-12-01 00:37:00|        NULL|        NULL|   NULL|                B00111|\n",
      "|              B00112|2024-12-01 00:46:01|2024-12-01 00:46:18|        NULL|          14|   NULL|                B00112|\n",
      "|              B00171|2024-12-01 00:18:00|2024-12-01 00:41:00|         157|          71|   NULL|                B02594|\n",
      "|              B00221|2024-12-01 00:06:49|2024-12-01 00:18:26|        NULL|         174|   NULL|                B00221|\n",
      "|              B00221|2024-12-01 00:30:07|2024-12-01 00:40:47|        NULL|         153|   NULL|                B00221|\n",
      "|              B00221|2024-12-01 00:58:06|2024-12-01 01:04:45|        NULL|         136|   NULL|                B00221|\n",
      "|              B00225|2024-12-01 00:45:48|2024-12-01 01:11:58|        NULL|         209|   NULL|                B03404|\n",
      "|              B00225|2024-12-01 00:33:54|2024-12-01 00:39:05|        NULL|          45|   NULL|                B03407|\n",
      "|              B00254|2024-12-01 00:13:42|2024-12-01 00:39:16|         237|         254|   NULL|                B00254|\n",
      "|              B00254|2024-12-01 00:38:24|2024-12-01 01:12:02|         230|         265|   NULL|                B03404|\n",
      "|              B00256|2024-12-01 00:10:32|2024-12-01 00:32:06|        NULL|        NULL|   NULL|                B00256|\n",
      "|              B00256|2024-12-01 00:10:03|2024-12-01 00:50:06|        NULL|        NULL|   NULL|                B00256|\n",
      "|              B00256|2024-12-01 00:28:12|2024-12-01 00:41:57|        NULL|        NULL|   NULL|                B00256|\n",
      "|              B00256|2024-12-01 00:39:29|2024-12-01 01:01:08|        NULL|        NULL|   NULL|                B00256|\n",
      "|              B00256|2024-12-01 00:23:06|2024-12-01 01:10:09|        NULL|        NULL|   NULL|                B00256|\n",
      "|              B00256|2024-12-01 00:47:01|2024-12-01 01:16:29|        NULL|        NULL|   NULL|                B02026|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all Parquet files in the folder\n",
    "df_fhv = spark.read.parquet(folder_path + 'fhv/fhv_tripdata_2024-*.parquet')\n",
    "\n",
    "number_rows = df_fhv.count()\n",
    "print('Number of rows: ' + str(number_rows))\n",
    "\n",
    "# Show the first few rows of the data\n",
    "df_fhv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHV EDA for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover duplicados (usar datetime, pickup location, destination location e total_amount)\n",
    "# remover numeros absurdos de trip_distance, total_amount usando algum metodo estatistico. Remover zeros, nulos e negativos\n",
    "# antes fazer analise desses numeros\n",
    "# validar se algumas das locations nao esta no mapping file\n",
    "# ver se VendorID, RatecodeID, store_and_fwd_flag, payment_type tem alguma coisa fora do que Ã© listado no dict\n",
    "# timestamps fora do comum\n",
    "# viagens extremamente longas\n",
    "# passager_count nulo\n",
    "# total_amount bate com a soma das individual charges fare_amount + extra + mta_tax + tip_amount + tolls_amount + improvement_surcharge + congestion_surcharge + Airport_fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
